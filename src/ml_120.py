# -*- coding: utf-8 -*-
"""ML_120.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HvI3kh94zOCuG4mWoSZpgMn-j2YdNNMo
"""


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


import seaborn as sns
from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, confusion_matrix, classification_report, silhouette_score, davies_bouldin_score, calinski_harabasz_score, fowlkes_mallows_score, mutual_info_score, adjusted_rand_score
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn import tree, neighbors, neural_network, ensemble, preprocessing
import streamlit as st
from sklearn.svm import SVC




# Binary encoding
def binary_encode_column(df, column_name):
  # Get unique values in the column and the number of bits required
  unique_values = df[column_name].unique()
  num_bits = int(np.ceil(np.log2(len(unique_values))))  # Minimum bits needed

  # Map each unique value to its binary representation
  binary_mapping = {val: format(i, f'0{num_bits}b') for i, val in enumerate(unique_values)}

  # Apply the mapping to create a new column with binary strings
  df[f"{column_name}_bin"] = df[column_name].map(binary_mapping)

  # Drop the original column if desired
  df = df.drop(columns=[column_name])

  return binary_mapping

def split(data):
  train, test = np.split(data.sample(frac=1), [int(0.8*len(data))])
  X_train = train.loc[:, train.columns != 'Difficulty']
  Y_train = train.loc[:, 'Difficulty']
  X_test = test.loc[:, test.columns != 'Difficulty']
  Y_test = test.loc[:, 'Difficulty']
  return X_train, X_test, Y_train, Y_test

def scale(X):
  scaler = StandardScaler()
  # Apply normalization
  X_scaled = scaler.fit_transform(X)
  return pd.DataFrame(X_scaled, columns=X.columns)



def create_difficulty(averageGrade, SD):
  difficulty = 0
  if averageGrade > 0 and averageGrade < 2:
    difficulty += 4
  elif averageGrade > 2 and averageGrade < 2.7:
    difficulty += 3
  elif averageGrade > 2.7 and averageGrade < 3.3:
    difficulty += 2
  elif averageGrade > 3.3 and averageGrade < 3.7:
    difficulty += 1
  if SD > 0.5:
    difficulty += 1
  if difficulty > 4:
    difficulty = 4
  return difficulty

def preprocess_data(filename = 'Grade_Distribution_Data.xlsx'):
    #Begin preprocessing here
    #There is a lot more that can be done with pandas for manipulating our data, this is just a start
    data = pd.read_excel('data/Grade_Distribution_Data.xlsx', sheet_name='AY2023 AY2024 Grade Distro')
    train_data = data.copy(deep="True")

    np.random.seed(100)

    train_data.drop(columns=['A', 'B', 'C', 'D', 'F', 'W', 'Trm Code'], inplace=True)
    train_data.rename(columns={'Academic Year': 'AcademicYear', 'Course Subject and Number': 'CourseSubjectandNumber', 'Average Grade': 'AverageGrade', 'Primary Instructor Name': 'PrimaryInstructorName'}, inplace=True)
    train_data = train_data[train_data.AcademicYear != "2022-23"]
    train_data = train_data[train_data.AverageGrade != 'Total']
    train_data.drop(columns=['AcademicYear'], inplace=True)
    train_data.dropna(inplace=True)

    #remove any labs, recitations, etc
    train_data = train_data[~train_data['Section'].str.contains(r'[0-9]', regex=True, na=False)]
    train_data.drop(columns=['Section'], inplace=True)

    #Begin converting all strings into ints, this require a lot of encoding and will be difficult for instructors
    train_data[['Subject', 'Number']] = train_data['CourseSubjectandNumber'].str.split(' ', n=1, expand=True)
    train_data.drop(columns=['CourseSubjectandNumber'], inplace=True)
    train_data.rename(columns={'PrimaryInstructorName': 'Instructor'}, inplace=True)
    #print(train_data.columns)

    columns_map = {'Subject': {'AE': 0, 'ARCH': 1, 'ECE': 2, 'ME': 3, 'NRE': 4, 'AE': 5, 'MP': 6}}
    train_data.replace(columns_map, inplace=True)

    #Apply difficulty to each column
    train_data.insert(len(train_data.columns), "Difficulty", 0)
    train_data['Difficulty'] = train_data.apply(lambda row: create_difficulty(row['AverageGrade'], row['Standard Deviation']),axis=1)

    # TODO: Recover name with the mapping
    # instructor_encode_map = binary_encode_column(train_data, 'Instructor')
    train_data.drop(columns=['Instructor'], inplace=True)

    #Removing binary and converting to ints/floats
    # train_data['Instructor_bin'] = train_data['Instructor_bin'].apply(lambda x: int(x, 2)).astype(float)
    # train_data['Number'] = train_data['Number'].astype(int)


    X = train_data.loc[:, train_data.columns != 'Difficulty']
    Y = train_data.loc[:, 'Difficulty']

    return X, Y, train_data

def Kmeans(X_train, y_train):

    # Standard scaling
    scaler = preprocessing.StandardScaler()
    scaled = scaler.fit_transform(X_train)
    X_train_scaled = pd.DataFrame(scaled, columns=X_train.columns)

    model = KMeans(n_clusters=5, random_state=0, n_init="auto") #Basic, no parameter tweaking really
    labels = model.fit_predict(X_train_scaled, y_train) # Need everything to be numbers instead of strings
    
    # metrics
    # KM_y_pred = cross_val_predict(model, X_train_scaled, y_train)
    fm_score = fowlkes_mallows_score(y_train, model.labels_)
    silhouette = silhouette_score(X_train, model.labels_)

    return labels, fm_score, silhouette

def SVM(X_train, X_test, Y_train, Y_test):
    # Kernels to test
    kernels = ['linear', 'rbf', 'poly', 'sigmoid']

    # Prepare data for plotting
    X_train_plot = X_train[['AverageGrade', 'Number']].values
    Y_train_plot = Y_train.values

    X_test_plot = X_test[['AverageGrade', 'Number']].values
    Y_test_plot = Y_test.values

    # Scale the features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_plot)
    X_test_scaled = scaler.transform(X_test_plot)

    # Streamlit interface
    st.title("SVM Kernel Comparison")

    # Iterate over kernels
    for kernel in kernels:
        st.subheader(f"{kernel.capitalize()} Kernel")
        
        # Train SVM with the current kernel
        svm = SVC(kernel=kernel, gamma='auto')
        svm.fit(X_train_scaled, Y_train_plot)

        # Create mesh to plot decision boundary
        x_min, x_max = X_train_plot[:, 0].min() - 1, X_train_plot[:, 0].max() + 1
        y_min, y_max = X_train_plot[:, 1].min() - 1, X_train_plot[:, 1].max() + 1
        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),
                             np.linspace(y_min, y_max, 200))

        # Transform mesh grid points
        xx_scaled = scaler.transform(np.c_[xx.ravel(), yy.ravel()])
        Z = svm.predict(xx_scaled)
        Z = Z.reshape(xx.shape)

        # Create plot
        fig, ax = plt.subplots(1, 2, figsize=(16, 6))

        # Plot decision boundary for training data
        ax[0].contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdBu)
        ax[0].scatter(X_train_plot[:, 0], X_train_plot[:, 1],
                      c=Y_train_plot, cmap=plt.cm.RdBu, edgecolor='black')
        ax[0].set_title(f'{kernel.capitalize()} Kernel - Training Data')
        ax[0].set_xlabel('Average Grade')
        ax[0].set_ylabel('Number')

        # Plot test data predictions
        Y_pred = svm.predict(X_test_scaled)
        ax[1].scatter(X_test_plot[:, 0], X_test_plot[:, 1],
                      c=Y_pred, cmap=plt.cm.RdBu, edgecolor='black')
        ax[1].set_title(f'{kernel.capitalize()} Kernel - Test Data Predictions')
        ax[1].set_xlabel('Average Grade')
        ax[1].set_ylabel('Number')

        # Display plots in Streamlit
        st.pyplot(fig)

    st.write("## Classification reports")
    st.image("results/kernels1.png")
    st.image("results/kernels2.png")

      
def RF(X_train, X_test, Y_train, Y_test):
  rf_clf = RandomForestClassifier(
      n_estimators=100,
      max_depth=10,
      random_state=42,
      class_weight="balanced"
  )
  
  rf_clf.fit(X_train, Y_train)
  Y_test_pred = rf_clf.predict(X_test)

  st.image("results/RF_results.png")

  feature_importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': rf_clf.feature_importances_
  }).sort_values(by='Importance', ascending=False)


  # Plot 1: Feature Importance
  plt.figure(figsize=(10, 6))
  sns.barplot(x='Importance', y='Feature', data=feature_importances)
  plt.title('Feature Importance in Random Forest')
  st.pyplot(plt)  # Display the plot in Streamlit

  # PCA for clustering
  st.image('results/PCA_RF_clustering.png')

def NN(X_train, X_test, Y_train, Y_test):
  # Data (replace X_train, X_test, Y_train, Y_test with your actual data)
  # For example, assume X_train, X_test, Y_train, Y_test are already loaded as DataFrame and Series.
  # X_train, X_test, Y_train, Y_test = ...

  # Create and train the Neural Network
  st.title("Neural Network Classifier")

  nn = MLPClassifier(
    hidden_layer_sizes=(16, 16),  # Two hidden layers with 16 and 16 neurons
    activation='relu',  # ReLU activation function
    solver='adam',      # Adam optimization algorithm
    max_iter=1000,      # Maximum number of iterations
    random_state=42     # For reproducibility
  )

  with st.spinner("Training the Neural Network..."):
    nn.fit(X_train, Y_train)

  # Display training data shape
  st.write("### Scaled Training Data:")
  st.write(X_train)

  # Predict
  Y_pred = nn.predict(X_test)

  # Display classification metrics
  st.write("### Classification Report:")
  st.image("results/nn.png")

  # Plot and display training loss curve
  st.write("### Neural Network Training Loss Curve:")
  fig, ax = plt.subplots()
  ax.plot(nn.loss_curve_)
  ax.set_title('Neural Network Training Loss')
  ax.set_xlabel('Iterations')
  ax.set_ylabel('Loss')
  st.pyplot(fig)

  # Feature importance
  if hasattr(nn, 'coefs_'):
    st.write("### Feature Importance (absolute mean of first layer weights):")
    feature_importance = np.abs(nn.coefs_[0]).mean(axis=1)
    feature_importance_df = pd.DataFrame({
        "Feature": X_train.columns,
        "Importance": feature_importance
    }).sort_values(by="Importance", ascending=False)
    st.write(feature_importance_df)